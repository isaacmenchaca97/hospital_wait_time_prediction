{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import pathlib\n",
    "import io\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import (\n",
    "    ProcessingInput, \n",
    "    ProcessingOutput, \n",
    "    ScriptProcessor\n",
    ")\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import (\n",
    "    ProcessingStep, \n",
    "    TrainingStep, \n",
    "    CreateModelStep\n",
    ")\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger, \n",
    "    ParameterFloat, \n",
    "    ParameterString, \n",
    "    ParameterBoolean\n",
    ")\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    ModelBiasCheckConfig, \n",
    "    ClarifyCheckStep, \n",
    "    ModelExplainabilityCheckConfig\n",
    ")\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    "    LambdaOutput,\n",
    "    LambdaOutputTypeEnum,\n",
    ")\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "\n",
    "from sagemaker.model_metrics import (\n",
    "    MetricsSource, \n",
    "    ModelMetrics, \n",
    "    FileSource\n",
    ")\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "\n",
    "from sagemaker.image_uris import retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a SageMaker Studio notebook and parameterize the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate AWS services session and client objects\n",
    "sess = sagemaker.Session()\n",
    "write_bucket = sess.default_bucket()\n",
    "write_prefix = \"hospital_wait_time_prediction\"\n",
    "\n",
    "region = sess.boto_region_name\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtime_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Fetch SageMaker execution role\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "# S3 locations used for parameterizing the notebook run\n",
    "read_bucket = sess.default_bucket()\n",
    "read_prefix = f\"{write_prefix}/raw\" \n",
    "\n",
    "# S3 location where raw data to be fetched from\n",
    "raw_data_key = f\"s3://{read_bucket}/{read_prefix}\"\n",
    "\n",
    "# S3 location where processed data to be uploaded\n",
    "processed_data_key = f\"{write_prefix}/processed\"\n",
    "\n",
    "# S3 location where train data to be uploaded\n",
    "train_data_key = f\"{write_prefix}/train\"\n",
    "\n",
    "# S3 location where validation data to be uploaded\n",
    "validation_data_key = f\"{write_prefix}/validation\"\n",
    "\n",
    "# S3 location where test data to be uploaded\n",
    "test_data_key = f\"{write_prefix}/test\"\n",
    "\n",
    "\n",
    "# Full S3 paths\n",
    "raw_data_uri = f\"{raw_data_key}/basesdedatos_50K.xlsx\"\n",
    "output_data_uri = f\"s3://{write_bucket}/{write_prefix}/\"\n",
    "scripts_uri = f\"s3://{write_bucket}/{write_prefix}/scripts\"\n",
    "estimator_output_uri = f\"s3://{write_bucket}/{write_prefix}/training_jobs\"\n",
    "processing_output_uri = f\"s3://{write_bucket}/{write_prefix}/processing_jobs\"\n",
    "model_eval_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_eval\"\n",
    "clarify_bias_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/bias_config\"\n",
    "clarify_explainability_config_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_monitor/explainability_config\"\n",
    "bias_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/bias\"\n",
    "explainability_report_output_uri = f\"s3://{write_bucket}/{write_prefix}/clarify_output/pipeline/explainability\"\n",
    "\n",
    "# Retrieve training image\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.3-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names of pipeline objects\n",
    "pipeline_name = \"HospitalWaitTimePredictionXGBPipeline\"\n",
    "pipeline_model_name = \"hospital-wait-time-prediction-xgb-pipeline\"\n",
    "model_package_group_name = \"hospital-wait-time-prediction-xgb-model-group\"\n",
    "base_job_name_prefix = \"hospital-wait-time-prediction\"\n",
    "endpoint_config_name = f\"{pipeline_model_name}-endpoint-config\"\n",
    "endpoint_name = f\"{pipeline_model_name}-endpoint\"\n",
    "\n",
    "# Set data parameters\n",
    "target_col = \"tiempo_total\"\n",
    "\n",
    "# Set instance types and counts\n",
    "process_instance_type = \"ml.m4.xlarge\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.m4.xlarge\"\n",
    "clarify_instance_count = 1\n",
    "clarify_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline input parameters\n",
    "\n",
    "# Set processing instance type\n",
    "process_instance_type_param = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=process_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance type\n",
    "train_instance_type_param = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=train_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance count\n",
    "train_instance_count_param = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\",\n",
    "    default_value=train_instance_count\n",
    ")\n",
    "\n",
    "# Set deployment instance type\n",
    "deploy_instance_type_param = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=predictor_instance_type,\n",
    ")\n",
    "\n",
    "# Set deployment instance count\n",
    "deploy_instance_count_param = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\",\n",
    "    default_value=predictor_instance_count\n",
    ")\n",
    "\n",
    "# Set Clarify check instance type\n",
    "clarify_instance_type_param = ParameterString(\n",
    "    name=\"ClarifyInstanceType\",\n",
    "    default_value=clarify_instance_type,\n",
    ")\n",
    "\n",
    "# Set model bias check params\n",
    "skip_check_model_bias_param = ParameterBoolean(\n",
    "    name=\"SkipModelBiasCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_bias_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelBiasBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_bias_param = ParameterString(\n",
    "    name=\"ModelBiasSuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model explainability check params\n",
    "skip_check_model_explainability_param = ParameterBoolean(\n",
    "    name=\"SkipModelExplainabilityCheck\", \n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "register_new_baseline_model_explainability_param = ParameterBoolean(\n",
    "    name=\"RegisterNewModelExplainabilityBaseline\",\n",
    "    default_value=False\n",
    ")\n",
    "\n",
    "supplied_baseline_constraints_model_explainability_param = ParameterString(\n",
    "    name=\"ModelExplainabilitySuppliedBaselineConstraints\", \n",
    "    default_value=\"\"\n",
    ")\n",
    "\n",
    "# Set model approval param\n",
    "model_approval_status_param = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"Approved\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import pathlib\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-ratio\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--validation-ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--test-ratio\", type=float, default=0.1)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    logger.info(f\"Received arguments {args}\")\n",
    "    \n",
    "    # Set local path prefix in the processing container\n",
    "    local_dir = \"/opt/ml/processing\"    \n",
    "    \n",
    "    input_data_path_claims = os.path.join(\"/opt/ml/processing/claims\", \"claims.csv\")\n",
    "    input_data_path_customers = os.path.join(\"/opt/ml/processing/customers\", \"customers.csv\")\n",
    "    \n",
    "    logger.info(\"Reading claims data from {}\".format(input_data_path_claims))\n",
    "    df_claims = pd.read_csv(input_data_path_claims)\n",
    "    \n",
    "    logger.info(\"Reading customers data from {}\".format(input_data_path_customers))\n",
    "    df_customers = pd.read_csv(input_data_path_customers)\n",
    "    \n",
    "    logger.debug(\"Formatting column names.\")\n",
    "    # Format column names\n",
    "    df_claims = df_claims.rename({c : c.lower().strip().replace(' ', '_') for c in df_claims.columns}, axis = 1)\n",
    "    df_customers = df_customers.rename({c : c.lower().strip().replace(' ', '_') for c in df_customers.columns}, axis = 1)\n",
    "    \n",
    "    logger.debug(\"Joining datasets.\")\n",
    "    # Join datasets\n",
    "    df_data = df_claims.merge(df_customers, on = 'policy_id', how = 'left')\n",
    "\n",
    "    # Drop selected columns not required for model building\n",
    "    df_data = df_data.drop(['customer_zip'], axis = 1)\n",
    "    \n",
    "    # Select Ordinal columns\n",
    "    ordinal_cols = [\"police_report_available\", \"policy_liability\", \"customer_education\"]\n",
    "\n",
    "    # Select categorical columns and filling with na\n",
    "    cat_cols_all = list(df_data.select_dtypes('object').columns)\n",
    "    cat_cols = [c for c in cat_cols_all if c not in ordinal_cols]\n",
    "    df_data[cat_cols] = df_data[cat_cols].fillna('na')\n",
    "    \n",
    "    logger.debug(\"One-hot encoding categorical columns.\")\n",
    "    # One-hot encoding categorical columns\n",
    "    df_data = pd.get_dummies(df_data, columns = cat_cols)\n",
    "    \n",
    "    logger.debug(\"Encoding ordinal columns.\")\n",
    "    # Ordinal encoding\n",
    "    mapping = {\n",
    "               \"Yes\": \"1\",\n",
    "               \"No\": \"0\" \n",
    "              }\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].map(mapping)\n",
    "    df_data['police_report_available'] = df_data['police_report_available'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"15/30\": \"0\",\n",
    "               \"25/50\": \"1\", \n",
    "               \"30/60\": \"2\",\n",
    "               \"100/200\": \"3\"\n",
    "              }\n",
    "    \n",
    "    df_data['policy_liability'] = df_data['policy_liability'].map(mapping)\n",
    "    df_data['policy_liability'] = df_data['policy_liability'].astype(float)\n",
    "\n",
    "    mapping = {\n",
    "               \"Below High School\": \"0\",\n",
    "               \"High School\": \"1\", \n",
    "               \"Associate\": \"2\",\n",
    "               \"Bachelor\": \"3\",\n",
    "               \"Advanced Degree\": \"4\"\n",
    "              }\n",
    "    \n",
    "    df_data['customer_education'] = df_data['customer_education'].map(mapping)\n",
    "    df_data['customer_education'] = df_data['customer_education'].astype(float)\n",
    "    \n",
    "    df_processed = df_data.copy()\n",
    "    df_processed.columns = [c.lower() for c in df_data.columns]\n",
    "    df_processed = df_processed.drop([\"policy_id\", \"customer_gender_unkown\"], axis=1)\n",
    "    \n",
    "    # Split into train, validation, and test sets\n",
    "    train_ratio = args.train_ratio\n",
    "    val_ratio = args.validation_ratio\n",
    "    test_ratio = args.test_ratio\n",
    "    \n",
    "    logger.debug(\"Splitting data into train, validation, and test sets\")\n",
    "    \n",
    "    y = df_processed['fraud']\n",
    "    X = df_processed.drop(['fraud'], axis = 1)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_ratio, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_ratio, random_state=42)\n",
    "\n",
    "    train_df = pd.concat([y_train, X_train], axis = 1)\n",
    "    val_df = pd.concat([y_val, X_val], axis = 1)\n",
    "    test_df = pd.concat([y_test, X_test], axis = 1)\n",
    "    dataset_df = pd.concat([y, X], axis = 1)\n",
    "    \n",
    "    logger.info(\"Train data shape after preprocessing: {}\".format(train_df.shape))\n",
    "    logger.info(\"Validation data shape after preprocessing: {}\".format(val_df.shape))\n",
    "    logger.info(\"Test data shape after preprocessing: {}\".format(test_df.shape))\n",
    "    \n",
    "    # Save processed datasets to the local paths in the processing container.\n",
    "    # SageMaker will upload the contents of these paths to S3 bucket\n",
    "    logger.debug(\"Writing processed datasets to container local path.\")\n",
    "    train_output_path = os.path.join(f\"{local_dir}/train\", \"train.csv\")\n",
    "    validation_output_path = os.path.join(f\"{local_dir}/val\", \"validation.csv\")\n",
    "    test_output_path = os.path.join(f\"{local_dir}/test\", \"test.csv\")\n",
    "    full_processed_output_path = os.path.join(f\"{local_dir}/full\", \"dataset.csv\")\n",
    "\n",
    "    logger.info(\"Saving train data to {}\".format(train_output_path))\n",
    "    train_df.to_csv(train_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving validation data to {}\".format(validation_output_path))\n",
    "    val_df.to_csv(validation_output_path, index=False)\n",
    "\n",
    "    logger.info(\"Saving test data to {}\".format(test_output_path))\n",
    "    test_df.to_csv(test_output_path, index=False)\n",
    "    \n",
    "    logger.info(\"Saving full processed data to {}\".format(full_processed_output_path))\n",
    "    dataset_df.to_csv(full_processed_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "# Upload processing script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"preprocessing.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/preprocessing.py\"\n",
    ")\n",
    "\n",
    "# Define the SKLearnProcessor configuration\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=process_instance_type,\n",
    "    base_job_name=f\"{base_job_name_prefix}-processing\",\n",
    ")\n",
    "\n",
    "# Define pipeline processing step\n",
    "process_step = ProcessingStep(\n",
    "    name=\"DataProcessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=claims_data_uri, destination=\"/opt/ml/processing/claims\"),\n",
    "        ProcessingInput(source=customers_data_uri, destination=\"/opt/ml/processing/customers\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/train_data\", output_name=\"train_data\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/validation_data\", output_name=\"validation_data\", source=\"/opt/ml/processing/val\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/test_data\", output_name=\"test_data\", source=\"/opt/ml/processing/test\"),\n",
    "        ProcessingOutput(destination=f\"{processing_output_uri}/processed_data\", output_name=\"processed_data\", source=\"/opt/ml/processing/full\")\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        \"--train-ratio\", \"0.8\", \n",
    "        \"--validation-ratio\", \"0.1\",\n",
    "        \"--test-ratio\", \"0.1\"\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/preprocessing.py\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
