{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Sklearn\n",
    "# ==============================================================================\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# MLflow\n",
    "# ==============================================================================\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# TensorFlow\n",
    "# ==============================================================================\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation, Input\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/isaacmenchaca97/hospital_wait_time_prediction/releases/download/v1.0.0/ARTICLE.tar.gz\n",
    "# !wget https://github.com/isaacmenchaca97/hospital_wait_time_prediction/releases/download/v1.0.0/EDIESCA.tar.gz\n",
    "\n",
    "#!tar -xzvf ARTICLE.tar.gz\n",
    "# !tar -xzvf EDIESCA.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target and features\n",
    "df_regression = pd.read_csv('../data/processed/EDIESCA.csv')\n",
    "X = df_regression.drop(columns=['tiempo_total'])\n",
    "y = df_regression['tiempo_total']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n"
     ]
    }
   ],
   "source": [
    "# Check if TPU is available\n",
    "try:\n",
    "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver() # No need to specify TPU address if on Colab\n",
    "  tf.config.experimental_connect_to_cluster(resolver)\n",
    "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "  strategy = tf.distribute.TPUStrategy(resolver)\n",
    "  print(\"Running on TPU\")\n",
    "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "except ValueError: # If TPU is not found, use default strategy\n",
    "  strategy = tf.distribute.get_strategy()\n",
    "  device_name = tf.test.gpu_device_name()\n",
    "  if device_name != '/device:GPU:0':\n",
    "    print(\"Running on CPU\")\n",
    "  else:\n",
    "    print(\"Running on GPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Input(shape=(X_train.shape[1],)))  # Use Input layer to define input shape\n",
    "\n",
    "  model.add(Dense(128, kernel_regularizer=l2(0.001)))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "\n",
    "  model.add(Dense(64, kernel_regularizer=l2(0.001)))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dropout(0.4))\n",
    "\n",
    "  model.add(Dense(32, kernel_regularizer=l2(0.001)))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Activation('relu'))\n",
    "\n",
    "  model.add(Dense(1))\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "Epoch 1/300\n",
      "2668/2675 [============================>.] - ETA: 0s - loss: 627.7030 - mae: 19.7160\n",
      "Epoch 1: val_loss improved from inf to 550.03839, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 14s 5ms/step - loss: 627.5956 - mae: 19.7154 - val_loss: 550.0384 - val_mae: 18.5646 - lr: 0.0050\n",
      "Epoch 2/300\n",
      "2675/2675 [==============================] - ETA: 0s - loss: 550.7484 - mae: 18.7049\n",
      "Epoch 2: val_loss improved from 550.03839 to 545.13977, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 550.7484 - mae: 18.7049 - val_loss: 545.1398 - val_mae: 18.4049 - lr: 0.0050\n",
      "Epoch 3/300\n",
      "2668/2675 [============================>.] - ETA: 0s - loss: 546.2878 - mae: 18.6103\n",
      "Epoch 3: val_loss improved from 545.13977 to 540.88947, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 546.3914 - mae: 18.6117 - val_loss: 540.8895 - val_mae: 18.5697 - lr: 0.0050\n",
      "Epoch 4/300\n",
      "2673/2675 [============================>.] - ETA: 0s - loss: 542.7695 - mae: 18.5458\n",
      "Epoch 4: val_loss improved from 540.88947 to 539.33997, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 542.7884 - mae: 18.5464 - val_loss: 539.3400 - val_mae: 18.5115 - lr: 0.0050\n",
      "Epoch 5/300\n",
      "2674/2675 [============================>.] - ETA: 0s - loss: 539.6042 - mae: 18.4703\n",
      "Epoch 5: val_loss improved from 539.33997 to 537.86450, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 539.6219 - mae: 18.4701 - val_loss: 537.8645 - val_mae: 18.4790 - lr: 0.0050\n",
      "Epoch 6/300\n",
      "2665/2675 [============================>.] - ETA: 0s - loss: 537.0987 - mae: 18.4211\n",
      "Epoch 6: val_loss improved from 537.86450 to 535.68719, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 537.3173 - mae: 18.4244 - val_loss: 535.6872 - val_mae: 18.5887 - lr: 0.0050\n",
      "Epoch 7/300\n",
      "2666/2675 [============================>.] - ETA: 0s - loss: 536.0438 - mae: 18.3863\n",
      "Epoch 7: val_loss improved from 535.68719 to 534.51978, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 536.0766 - mae: 18.3861 - val_loss: 534.5198 - val_mae: 18.3805 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "2667/2675 [============================>.] - ETA: 0s - loss: 535.1094 - mae: 18.3652\n",
      "Epoch 8: val_loss did not improve from 534.51978\n",
      "2675/2675 [==============================] - 12s 4ms/step - loss: 534.9816 - mae: 18.3632 - val_loss: 536.2058 - val_mae: 18.1439 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "2673/2675 [============================>.] - ETA: 0s - loss: 534.1754 - mae: 18.3414\n",
      "Epoch 9: val_loss improved from 534.51978 to 533.68512, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 534.1873 - mae: 18.3418 - val_loss: 533.6851 - val_mae: 18.3755 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "2663/2675 [============================>.] - ETA: 0s - loss: 533.4411 - mae: 18.3216\n",
      "Epoch 10: val_loss did not improve from 533.68512\n",
      "2675/2675 [==============================] - 11s 4ms/step - loss: 533.5376 - mae: 18.3234 - val_loss: 534.6274 - val_mae: 18.5484 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "2673/2675 [============================>.] - ETA: 0s - loss: 533.2655 - mae: 18.3123\n",
      "Epoch 11: val_loss did not improve from 533.68512\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 533.1630 - mae: 18.3099 - val_loss: 533.7970 - val_mae: 18.1457 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "2663/2675 [============================>.] - ETA: 0s - loss: 532.6288 - mae: 18.2915\n",
      "Epoch 12: val_loss did not improve from 533.68512\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 532.4188 - mae: 18.2873 - val_loss: 534.7914 - val_mae: 18.1259 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "2675/2675 [==============================] - ETA: 0s - loss: 531.6521 - mae: 18.2745\n",
      "Epoch 13: val_loss did not improve from 533.68512\n",
      "2675/2675 [==============================] - 12s 4ms/step - loss: 531.6521 - mae: 18.2745 - val_loss: 537.8555 - val_mae: 18.0229 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "2675/2675 [==============================] - ETA: 0s - loss: 531.4453 - mae: 18.2574\n",
      "Epoch 14: val_loss did not improve from 533.68512\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "2675/2675 [==============================] - 12s 4ms/step - loss: 531.4453 - mae: 18.2574 - val_loss: 533.8345 - val_mae: 18.2960 - lr: 0.0050\n",
      "Epoch 15/300\n",
      "2662/2675 [============================>.] - ETA: 0s - loss: 527.3383 - mae: 18.1821\n",
      "Epoch 15: val_loss improved from 533.68512 to 532.18201, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 527.2357 - mae: 18.1790 - val_loss: 532.1820 - val_mae: 18.2406 - lr: 0.0025\n",
      "Epoch 16/300\n",
      "2668/2675 [============================>.] - ETA: 0s - loss: 525.8134 - mae: 18.1415\n",
      "Epoch 16: val_loss did not improve from 532.18201\n",
      "2675/2675 [==============================] - 12s 4ms/step - loss: 525.8382 - mae: 18.1432 - val_loss: 532.4268 - val_mae: 18.1852 - lr: 0.0025\n",
      "Epoch 17/300\n",
      "2674/2675 [============================>.] - ETA: 0s - loss: 524.5550 - mae: 18.1194\n",
      "Epoch 17: val_loss did not improve from 532.18201\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 524.5195 - mae: 18.1190 - val_loss: 532.2744 - val_mae: 18.2593 - lr: 0.0025\n",
      "Epoch 18/300\n",
      "2672/2675 [============================>.] - ETA: 0s - loss: 523.8276 - mae: 18.1110\n",
      "Epoch 18: val_loss did not improve from 532.18201\n",
      "2675/2675 [==============================] - 11s 4ms/step - loss: 523.8533 - mae: 18.1118 - val_loss: 532.3892 - val_mae: 18.2212 - lr: 0.0025\n",
      "Epoch 19/300\n",
      "2670/2675 [============================>.] - ETA: 0s - loss: 524.0919 - mae: 18.1055\n",
      "Epoch 19: val_loss improved from 532.18201 to 531.98541, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 11s 4ms/step - loss: 524.0049 - mae: 18.1042 - val_loss: 531.9854 - val_mae: 18.2025 - lr: 0.0025\n",
      "Epoch 20/300\n",
      "2670/2675 [============================>.] - ETA: 0s - loss: 523.1659 - mae: 18.0913\n",
      "Epoch 20: val_loss improved from 531.98541 to 531.92896, saving model to best_model.keras\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 523.2645 - mae: 18.0929 - val_loss: 531.9290 - val_mae: 18.2545 - lr: 0.0025\n",
      "Epoch 21/300\n",
      "2675/2675 [==============================] - ETA: 0s - loss: 523.0790 - mae: 18.0948\n",
      "Epoch 21: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 523.0790 - mae: 18.0948 - val_loss: 533.7029 - val_mae: 18.3709 - lr: 0.0025\n",
      "Epoch 22/300\n",
      "2673/2675 [============================>.] - ETA: 0s - loss: 522.7749 - mae: 18.0876\n",
      "Epoch 22: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 522.7700 - mae: 18.0875 - val_loss: 532.5667 - val_mae: 18.2108 - lr: 0.0025\n",
      "Epoch 23/300\n",
      "2663/2675 [============================>.] - ETA: 0s - loss: 522.6152 - mae: 18.0736\n",
      "Epoch 23: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 522.4796 - mae: 18.0710 - val_loss: 532.7001 - val_mae: 18.2373 - lr: 0.0025\n",
      "Epoch 24/300\n",
      "2674/2675 [============================>.] - ETA: 0s - loss: 521.9196 - mae: 18.0653\n",
      "Epoch 24: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 521.9047 - mae: 18.0648 - val_loss: 532.9556 - val_mae: 18.2954 - lr: 0.0025\n",
      "Epoch 25/300\n",
      "2675/2675 [==============================] - ETA: 0s - loss: 522.1940 - mae: 18.0727\n",
      "Epoch 25: val_loss did not improve from 531.92896\n",
      "\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 522.1940 - mae: 18.0727 - val_loss: 533.5657 - val_mae: 18.3901 - lr: 0.0025\n",
      "Epoch 26/300\n",
      "2675/2675 [==============================] - ETA: 0s - loss: 519.2468 - mae: 18.0123\n",
      "Epoch 26: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 4ms/step - loss: 519.2468 - mae: 18.0123 - val_loss: 532.5237 - val_mae: 18.2010 - lr: 0.0012\n",
      "Epoch 27/300\n",
      "2664/2675 [============================>.] - ETA: 0s - loss: 518.5200 - mae: 17.9903\n",
      "Epoch 27: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 11s 4ms/step - loss: 518.5996 - mae: 17.9909 - val_loss: 533.2917 - val_mae: 18.2390 - lr: 0.0012\n",
      "Epoch 28/300\n",
      "2672/2675 [============================>.] - ETA: 0s - loss: 517.3002 - mae: 17.9760\n",
      "Epoch 28: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 4ms/step - loss: 517.3022 - mae: 17.9767 - val_loss: 533.9277 - val_mae: 18.2843 - lr: 0.0012\n",
      "Epoch 29/300\n",
      "2670/2675 [============================>.] - ETA: 0s - loss: 517.0576 - mae: 17.9644\n",
      "Epoch 29: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 517.1047 - mae: 17.9653 - val_loss: 533.8024 - val_mae: 18.3280 - lr: 0.0012\n",
      "Epoch 30/300\n",
      "2667/2675 [============================>.] - ETA: 0s - loss: 516.8290 - mae: 17.9610\n",
      "Epoch 30: val_loss did not improve from 531.92896\n",
      "\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 516.9036 - mae: 17.9627 - val_loss: 534.0148 - val_mae: 18.2152 - lr: 0.0012\n",
      "Epoch 31/300\n",
      "2667/2675 [============================>.] - ETA: 0s - loss: 514.9155 - mae: 17.9263\n",
      "Epoch 31: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 514.7571 - mae: 17.9242 - val_loss: 533.8348 - val_mae: 18.2251 - lr: 6.2500e-04\n",
      "Epoch 32/300\n",
      "2669/2675 [============================>.] - ETA: 0s - loss: 515.1631 - mae: 17.9348\n",
      "Epoch 32: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 515.1738 - mae: 17.9348 - val_loss: 533.4518 - val_mae: 18.1838 - lr: 6.2500e-04\n",
      "Epoch 33/300\n",
      "2671/2675 [============================>.] - ETA: 0s - loss: 514.5497 - mae: 17.9177\n",
      "Epoch 33: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 514.4971 - mae: 17.9168 - val_loss: 534.1887 - val_mae: 18.2518 - lr: 6.2500e-04\n",
      "Epoch 34/300\n",
      "2671/2675 [============================>.] - ETA: 0s - loss: 514.3333 - mae: 17.9162\n",
      "Epoch 34: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 13s 5ms/step - loss: 514.3209 - mae: 17.9163 - val_loss: 534.2840 - val_mae: 18.2436 - lr: 6.2500e-04\n",
      "Epoch 35/300\n",
      "2672/2675 [============================>.] - ETA: 0s - loss: 513.3605 - mae: 17.8931\n",
      "Epoch 35: val_loss did not improve from 531.92896\n",
      "\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 513.4205 - mae: 17.8936 - val_loss: 534.3449 - val_mae: 18.1943 - lr: 6.2500e-04\n",
      "Epoch 36/300\n",
      "2669/2675 [============================>.] - ETA: 0s - loss: 512.1738 - mae: 17.8735\n",
      "Epoch 36: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 512.1074 - mae: 17.8721 - val_loss: 534.2373 - val_mae: 18.2602 - lr: 3.1250e-04\n",
      "Epoch 37/300\n",
      "2665/2675 [============================>.] - ETA: 0s - loss: 513.1136 - mae: 17.8884\n",
      "Epoch 37: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 11s 4ms/step - loss: 512.9989 - mae: 17.8874 - val_loss: 534.5604 - val_mae: 18.2123 - lr: 3.1250e-04\n",
      "Epoch 38/300\n",
      "2670/2675 [============================>.] - ETA: 0s - loss: 512.3020 - mae: 17.8726\n",
      "Epoch 38: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 4ms/step - loss: 512.2715 - mae: 17.8713 - val_loss: 534.8082 - val_mae: 18.2463 - lr: 3.1250e-04\n",
      "Epoch 39/300\n",
      "2672/2675 [============================>.] - ETA: 0s - loss: 512.4373 - mae: 17.8804\n",
      "Epoch 39: val_loss did not improve from 531.92896\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 512.3929 - mae: 17.8800 - val_loss: 534.4474 - val_mae: 18.2456 - lr: 3.1250e-04\n",
      "Epoch 40/300\n",
      "2662/2675 [============================>.] - ETA: 0s - loss: 512.9714 - mae: 17.8859\n",
      "Epoch 40: val_loss did not improve from 531.92896\n",
      "\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "2675/2675 [==============================] - 12s 5ms/step - loss: 512.8456 - mae: 17.8842 - val_loss: 535.1605 - val_mae: 18.2069 - lr: 3.1250e-04\n"
     ]
    }
   ],
   "source": [
    "# Free session memori\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Set TensorBoard\n",
    "%load_ext tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Configurar callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n",
    "model_checkpoint = ModelCheckpoint('../models/best_model.keras', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "with strategy.scope():\n",
    "# with tf.device('/device:GPU:0'):\n",
    "  model = create_model()\n",
    "  initial_learning_rate = 0.005\n",
    "  optimizer = RMSprop(learning_rate=initial_learning_rate)\n",
    "  model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "  # Fit model\n",
    "  history = model.fit(\n",
    "      X_train, y_train,\n",
    "      validation_data=(X_test, y_test),\n",
    "      epochs=300,\n",
    "      batch_size=64,\n",
    "      callbacks=[model_checkpoint, tensorboard_callback, early_stopping, reduce_lr],\n",
    "      verbose=1,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 531.9284057617188\n",
      "Test accuracy: 18.254505157470703\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 48101), started 1:57:03 ago. (Use '!kill 48101' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model_ann = load_model(\"../models/best_model.keras\")\n",
    "\n",
    "# Predicción\n",
    "y_pred = model_ann.predict(X_test)\n",
    "\n",
    "# Métricas de evaluación\n",
    "print(f\"Root Mean Squared Error (RMSE): {np.sqrt(mean_squared_error(y_test, y_pred))}\")\n",
    "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred)}\")\n",
    "print(f\"Coeficiente de determinación (R^2): {r2_score(y_test, y_pred)}\")\n",
    "print(f'Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred)}')\n",
    "\n",
    "# Mostrar algunas predicciones junto con los valores reales\n",
    "predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten()})\n",
    "print(predictions.head(10))\n",
    "\n",
    "# Gráfico de dispersión de predicciones vs valores reales\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_pred, edgecolor='k', alpha=0.7, color=\"#3f7f93\")\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)\n",
    "plt.xlabel('Real Value (minutes)')\n",
    "plt.ylabel('Prediction (minutes)')\n",
    "plt.title('MLP Results')\n",
    "plt.show()\n",
    "\n",
    "# Imprimir las claves de history.history\n",
    "print(f\"loss {min(history.history['loss'])}\")\n",
    "print(f\"val_loss {min(history.history['val_loss'])}\")\n",
    "print(f\"mae {min(history.history['mae'])}\")\n",
    "print(f\"val_mae {min(history.history['val_mae'])}\")\n",
    "\n",
    "# Crear subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Graficar la pérdida de entrenamiento\n",
    "axs[0, 0].plot(history.history['loss'], label='Loss of Training', color=\"black\")\n",
    "axs[0, 0].set_xlabel('Epoch')\n",
    "axs[0, 0].set_ylabel('MSE')\n",
    "axs[0, 0].set_title('Loss of Training')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Graficar la pérdida de validación\n",
    "axs[0, 1].plot(history.history['val_loss'], label='Loss of Validation', color=\"red\")\n",
    "axs[0, 1].set_xlabel('Epoch')\n",
    "axs[0, 1].set_ylabel('MSE')\n",
    "axs[0, 1].set_title('Loss of Validation')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Graficar MAE de entrenamiento\n",
    "axs[1, 0].plot(history.history['mae'], label='MAE of Training', color=\"black\")\n",
    "axs[1, 0].set_xlabel('Epoch')\n",
    "axs[1, 0].set_ylabel('MAE')\n",
    "axs[1, 0].set_title('MAE of Training')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Graficar MAE de validación\n",
    "axs[1, 1].plot(history.history['val_mae'], label='MAE of validation', color=\"red\")\n",
    "axs[1, 1].set_xlabel('Epoch')\n",
    "axs[1, 1].set_ylabel('MAE')\n",
    "axs[1, 1].set_title('MAE of validation')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Ajustar diseño para evitar solapamiento\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
