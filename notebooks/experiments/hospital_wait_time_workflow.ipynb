{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.deserializers import CSVDeserializer\n",
    "from sagemaker.drift_check_baselines import DriftCheckBaselines\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.lambda_helper import Lambda\n",
    "from sagemaker.model_metrics import FileSource, MetricsSource, ModelMetrics\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.check_job_config import CheckJobConfig\n",
    "from sagemaker.workflow.clarify_check_step import (\n",
    "    ClarifyCheckStep,\n",
    "    ModelBiasCheckConfig,\n",
    "    ModelExplainabilityCheckConfig,\n",
    ")\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.lambda_step import (\n",
    "    LambdaStep,\n",
    ")\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterBoolean,\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.steps import CreateModelStep, ProcessingStep, TrainingStep\n",
    "from sagemaker.xgboost.estimator import XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up a SageMaker Studio notebook and parameterize the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate AWS services session and client objects\n",
    "sess = sagemaker.Session()\n",
    "write_bucket = sess.default_bucket()\n",
    "write_prefix = \"hospital_wait_time_prediction\"\n",
    "\n",
    "read_bucket = sess.default_bucket()\n",
    "read_prefix = \"hospital_wait_time_prediction/raw\"\n",
    "\n",
    "region = sess.boto_region_name\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "sm_runtiome_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# Fetch SageMaker execution role\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Full S3 paths\n",
    "hospital_data_uri = f\"s3://{read_bucket}/{read_prefix}/basesdedatos_50K.xlsx\"\n",
    "output_data_uri = f\"s3://{write_bucket}/{write_prefix}/\"\n",
    "scripts_uri = f\"s3://{write_bucket}/{write_prefix}/scripts\"\n",
    "estimator_output_uri = f\"s3://{write_bucket}/{write_prefix}/training_jobs\"\n",
    "processing_output_uri = f\"s3://{write_bucket}/{write_prefix}/processing_jobs\"\n",
    "model_eval_output_uri = f\"s3://{write_bucket}/{write_prefix}/model_eval\"\n",
    "\n",
    "# Retrieve training image\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.3-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set names of pipeline objects\n",
    "pipeline_name = \"HospitalWaitTimePredPipeline\"\n",
    "pipeline_model_name = \"hospital-wait-time-pred-pipeline\"\n",
    "model_package_group_name = \"hospital-wait-time-pred-model-group\"\n",
    "base_job_name_prefix = \"hospital-pred\"\n",
    "endpoint_config_name = f\"{pipeline_model_name}-endpoint-config\"\n",
    "endpoint_name = f\"{pipeline_model_name}-endpoint\"\n",
    "\n",
    "# Set instance types and counts\n",
    "process_instance_type = \"ml.m4.xlarge\"\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.m4.xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline input parameters\n",
    "\n",
    "# Set processing instance type\n",
    "process_instance_type_param = ParameterString(\n",
    "    name=\"ProcessingInstanceType\",\n",
    "    default_value=process_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance type\n",
    "train_instance_type_param = ParameterString(\n",
    "    name=\"TrainingInstanceType\",\n",
    "    default_value=train_instance_type,\n",
    ")\n",
    "\n",
    "# Set training instance count\n",
    "train_instance_count_param = ParameterInteger(\n",
    "    name=\"TrainingInstanceCount\", default_value=train_instance_count\n",
    ")\n",
    "\n",
    "# Set deployment instance type\n",
    "deploy_instance_type_param = ParameterString(\n",
    "    name=\"DeployInstanceType\",\n",
    "    default_value=predictor_instance_type,\n",
    ")\n",
    "\n",
    "# Set deployment instance count\n",
    "deploy_instance_count_param = ParameterInteger(\n",
    "    name=\"DeployInstanceCount\", default_value=predictor_instance_count\n",
    ")\n",
    "\n",
    "# Set model approval param\n",
    "model_approval_status_param = ParameterString(name=\"ModelApprovalStatus\", default_value=\"Approved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile process.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "\n",
    "import gender_guesser.detector as gender\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse job arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train-ratio\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--validation-ratio\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--test-ratio\", type=float, default=0.1)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    logger.info(f\"Received arguments {args}\")\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load data from input path\"\"\"\n",
    "    input_path = os.path.join(\"/opt/ml/processing/input\", \"basededatos_50K.xlsx\")\n",
    "    logger.info(f\"Loading data from {input_path}\")\n",
    "    df = pd.read_excel(input_path)\n",
    "    logger.info(f\"Loaded dataset with shape: {df.shape}\")\n",
    "    logger.info(f\"Dataset info: {df.info()}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_duplicates(df):\n",
    "    \"\"\"Remove duplicate records\"\"\"\n",
    "    logger.info(\"Cleaning duplicates\")\n",
    "    initial_len = len(df)\n",
    "    df = df.drop_duplicates()\n",
    "    logger.info(\n",
    "        f\"Number of duplicates:\\t\\t {initial_len - len(df)} - {(initial_len - len(df)) / len(df) * 100:.1f}%\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values in the dataset\"\"\"\n",
    "    logger.info(\"Handling missing values\")\n",
    "    initial_len = len(df)\n",
    "    df = df.dropna()\n",
    "    logger.info(\n",
    "        f\"Number of missing values:\\t {initial_len - len(df)} - {(initial_len - len(df)) / len(df) * 100:.1f}%\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def absolute_values(df):\n",
    "    \"\"\"Convert values to absolute values\"\"\"\n",
    "    logger.info(\"Converting values to absolute values\")\n",
    "    df[\"tiempo_espera_triage\"] = df[\"tiempo_espera_triage\"].abs()\n",
    "    df[\"tiempo_en_triage\"] = df[\"tiempo_en_triage\"].abs()\n",
    "    df[\"tiempo_espera_despuestriage\"] = df[\"tiempo_espera_despuestriage\"].abs()\n",
    "    df[\"tiempo_en_consulta\"] = df[\"tiempo_en_consulta\"].abs()\n",
    "    df[\"edad\"] = df[\"edad\"].abs()\n",
    "\n",
    "    # create a column with the total time\n",
    "    df[\"tiempo_total\"] = (\n",
    "        df[\"tiempo_espera_triage\"]\n",
    "        + df[\"tiempo_en_triage\"]\n",
    "        + df[\"tiempo_espera_despuestriage\"]\n",
    "        + df[\"tiempo_en_consulta\"]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(df, column):\n",
    "    \"\"\"Remove outliers from a column\"\"\"\n",
    "    logger.info(f\"Removing outliers from {column}\")\n",
    "    initial_len = len(df)\n",
    "\n",
    "    if column == \"edad\":\n",
    "        logger.info(\"Standard age values\")\n",
    "        df.rename(columns={\"Edad1\": \"edad_unidad\"}, inplace=True)\n",
    "        # convert column edad_unidad to numeric.\n",
    "        df[\"edad_unidad\"] = df[\"edad_unidad\"].replace(\n",
    "            {\"AÃ±os\": 1, \"Meses\": 1 / 12, \"Dias\": 1 / 365}\n",
    "        )\n",
    "        # standardize the value of the age\n",
    "        df[\"edad\"] = df[\"edad\"] * df[\"edad_unidad\"]\n",
    "\n",
    "    # Calculate the percentiles\n",
    "    twenty_fifth = df[column].quantile(0.25)\n",
    "    seventy_fifth = df[column].quantile(0.75)\n",
    "\n",
    "    # Obtain IQR\n",
    "    iqr = seventy_fifth - twenty_fifth\n",
    "\n",
    "    # Upper and lower thresholds\n",
    "    upper = seventy_fifth + (1.5 * iqr)\n",
    "    lower = twenty_fifth - (1.5 * iqr)\n",
    "\n",
    "    # Subset the dataset\n",
    "    outliers = df[(df[column] < lower) | (df[column] > upper)]\n",
    "\n",
    "    logger.info(\n",
    "        f\"Number of outliers for {column}:\\t {len(outliers)} - {(len(outliers) / initial_len * 100):.1f}%\"\n",
    "    )\n",
    "\n",
    "    df = df.drop(outliers.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def gender_encoding(df):\n",
    "    \"\"\"Encode gender\"\"\"\n",
    "    logger.info(\"Encoding gender\")\n",
    "    initial_len = len(df)\n",
    "    # Create an gender detector object\n",
    "    d = gender.Detector()\n",
    "\n",
    "    def obtener_genero(nombre):\n",
    "        # setting the second name, there is a total categorization between 'Ambiguo' y 'Desconocido' of 34,535 names\n",
    "        nombre = nombre.split()[-1]\n",
    "        nombre = nombre.title()\n",
    "        # if the name is 'Femenino' or 'Masculino' return the name\n",
    "        # total categorization between 'Ambiguo' y 'Desconocido' of 27,709 names. Recover 6,826 names\n",
    "        if nombre == \"Femenino\" or nombre == \"Masculino\":\n",
    "            return nombre\n",
    "        else:\n",
    "            return d.get_gender(nombre)\n",
    "\n",
    "    df[\"genero\"] = df[\"nombre\"].apply(obtener_genero)\n",
    "\n",
    "    df[\"genero\"] = df[\"genero\"].replace(\n",
    "        {\n",
    "            \"male\": \"Masculino\",\n",
    "            \"female\": \"Femenino\",\n",
    "            \"andy\": \"Ambiguo\",\n",
    "            \"unknown\": \"Desconocido\",\n",
    "            \"mostly_male\": \"Masculino\",\n",
    "            \"mostly_female\": \"Femenino\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_ambiguo = df[(df[\"genero\"] == \"Ambiguo\") | (df[\"genero\"] == \"Desconocido\")]\n",
    "\n",
    "    logger.info(\n",
    "        f\"Number of unknown:\\t {len(df_ambiguo)} - {len(df_ambiguo) / initial_len * 100:.1f}%\"\n",
    "    )\n",
    "\n",
    "    df = df.drop(df_ambiguo.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def encode_categorical(df):\n",
    "    \"\"\"Encode categorical variables\"\"\"\n",
    "    logger.info(\"Encoding categorical variables\")\n",
    "    # Encode clasification 'ROJO': 0, 'AMARILLO': 1, 'VERDE': 2\n",
    "    df[\"clasificacion_encode\"] = df[\"clasificacion\"].replace(\n",
    "        {\"ROJO\": 0, \"AMARILLO\": 1, \"VERDE\": 2}\n",
    "    )\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    # Aplicar el codificador a la columna 'genero'\n",
    "    genero_encoded = encoder.fit_transform(df[[\"genero\"]])\n",
    "    # Convertir la salida a un DataFrame\n",
    "    genero_encoded_df = pd.DataFrame(\n",
    "        genero_encoded, columns=encoder.get_feature_names_out([\"genero\"]), index=df.index\n",
    "    )\n",
    "    # Concatenar las columnas codificadas con el DataFrame original\n",
    "    df = pd.concat([df, genero_encoded_df], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_dates(df):\n",
    "    \"\"\"Extract features from date columns\"\"\"\n",
    "    logger.info(\"Extracting time features from date columns\")\n",
    "    # Extract caracteristics from date\n",
    "    df[\"hora\"] = df[\"Fecha\"].dt.hour\n",
    "    df[\"minuto\"] = df[\"Fecha\"].dt.minute\n",
    "    df[\"mes\"] = df[\"Fecha\"].dt.month\n",
    "    df[\"dia\"] = df[\"Fecha\"].dt.day\n",
    "    df[\"dia_semana\"] = df[\"Fecha\"].dt.dayofweek\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    \"\"\"Remove punctuation from the dataset\"\"\"\n",
    "    logger.info(\"Removing punctuation from the dataset\")\n",
    "    initial_len = len(df)\n",
    "    # clean text\n",
    "    df_not_string = df[~df[\"Dx\"].apply(lambda x: isinstance(x, str))]\n",
    "    logger.info(\n",
    "        f\"Number of not string values:\\t {len(df_not_string)} - {len(df_not_string) / initial_len * 100:.3f}%\"\n",
    "    )\n",
    "    df = df.drop(df_not_string.index)\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "        \"\"\"custom function to remove the punctuation\"\"\"\n",
    "        return text.translate(str.maketrans(\"\", \"\", string.punctuation + \"1\" + \"2\" + \"3\"))\n",
    "\n",
    "    df[\"Dx\"] = df[\"Dx\"].apply(lambda text: remove_punctuation(text))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_empty_string(df):\n",
    "    \"\"\"Remove empty string from the dataset\"\"\"\n",
    "    logger.info(\"Removing empty string from the dataset\")\n",
    "    initial_len = len(df)\n",
    "    df_empty_string = df[df[\"Dx\"] == \"\"]\n",
    "    logger.info(\n",
    "        f\"Number of empty string:\\t {len(df_empty_string)} - {len(df_empty_string) / initial_len * 100:.1f}%\"\n",
    "    )\n",
    "    df = df.drop(df_empty_string.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_meaningless_strings(df):\n",
    "    \"\"\"Delete meaningless strings from the dataset\"\"\"\n",
    "    logger.info(\"Deleting meaningless strings from the dataset\")\n",
    "    initial_len = len(df)\n",
    "\n",
    "    df[\"Dx\"] = df[\"Dx\"].str.lower()\n",
    "\n",
    "    def search_meaningless_str(text):\n",
    "        found = re.search(\"^x+$\", text)\n",
    "        if found is not None:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    df_x_strings = df[df[\"Dx\"].apply(search_meaningless_str)]\n",
    "    logger.info(\n",
    "        f\"Number of meaningless string:\\t {len(df_x_strings)} - {len(df_x_strings) / initial_len * 100:.1f}%\"\n",
    "    )\n",
    "\n",
    "    df = df.drop(df_x_strings.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_embeddings(df):\n",
    "    \"\"\"Generate embeddings for the dataset\"\"\"\n",
    "    # Create a TaggedDocument object\n",
    "    tagged_data = [\n",
    "        TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(df[\"Dx\"])\n",
    "    ]\n",
    "\n",
    "    # Create a Doc2Vec model\n",
    "    model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=40)\n",
    "\n",
    "    df[\"doc2vec\"] = [model.dv[str(i)] for i in range(len(tagged_data))]\n",
    "\n",
    "    df_doc2vec = pd.DataFrame(df[\"doc2vec\"].tolist(), index=df.index)\n",
    "    df = pd.concat([df.drop(\"doc2vec\", axis=1), df_doc2vec], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Main preprocessing function\"\"\"\n",
    "    args = parse_args()\n",
    "\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "\n",
    "    # Clean duplicates\n",
    "    df = clean_duplicates(df)\n",
    "\n",
    "    # Handle missing values\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    # Convert values to absolute values\n",
    "    df = absolute_values(df)\n",
    "\n",
    "    # Outliers for 'tiempo_total' (IQR) interquartile range\n",
    "    df = remove_outliers(df, \"tiempo_total\")\n",
    "\n",
    "    # Outliers for 'edad' (IQR) interquartile range\n",
    "    df = remove_outliers(df, \"edad\")\n",
    "\n",
    "    # Obtain gender\n",
    "    df = gender_encoding(df)\n",
    "\n",
    "    # Encode categorical variables\n",
    "    df = encode_categorical(df)\n",
    "\n",
    "    # Extract time features\n",
    "    df = process_dates(df)\n",
    "\n",
    "    # Removal of punctuation\n",
    "    df = remove_punctuation(df)\n",
    "\n",
    "    # Remove empty string\n",
    "    df = remove_empty_string(df)\n",
    "\n",
    "    # Delete meaningless strings\n",
    "    df = delete_meaningless_strings(df)\n",
    "\n",
    "    # Generate embeddings\n",
    "    df = generate_embeddings(df)\n",
    "\n",
    "    df = df.drop(\n",
    "        columns=[\"nombre\", \"apat\", \"amat\", \"Fecha\", \"Dx\", \"edad_unidad\", \"genero\", \"clasificacion\"]\n",
    "    )\n",
    "    logger.info(f\"Dropped columns: {df.columns}\")\n",
    "    logger.info(f\"Processed data shape: {df.shape}\")\n",
    "\n",
    "    # Split into train, validation, and test sets\n",
    "    logger.debug(\"Splitting data into train, validation, and test sets\")\n",
    "    x = df.drop(columns=[\"tiempo_total\"])\n",
    "    y = df[\"tiempo_total\"]\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x, y, test_size=args.test_ratio, random_state=42\n",
    "    )\n",
    "    x_val, x_test, y_val, y_test = train_test_split(\n",
    "        x_val, y_val, test_size=args.validation_ratio, random_state=42\n",
    "    )\n",
    "\n",
    "    train_df = pd.concat([y_train, x_train], axis=1)\n",
    "    val_df = pd.concat([y_val, x_val], axis=1)\n",
    "    test_df = pd.concat([y_test, x_test], axis=1)\n",
    "    dataset_df = pd.concat([y, x], axis=1)\n",
    "\n",
    "    logger.info(\"Train data shape after preprocessing: {}\".format(train_df.shape))\n",
    "    logger.info(\"Validation data shape after preprocessing: {}\".format(val_df.shape))\n",
    "    logger.info(\"Test data shape after preprocessing: {}\".format(test_df.shape))\n",
    "\n",
    "    # Save processed datasets to the local paths in the processing container.\n",
    "    # SageMaker will upload the contents of these paths to S3 bucket\n",
    "    local_dir = \"/opt/ml/processing\"\n",
    "    logger.debug(\"Writing processed datasets to container local path.\")\n",
    "    train_output_path = os.path.join(f\"{local_dir}/train\", \"train.csv\")\n",
    "    validation_output_path = os.path.join(f\"{local_dir}/val\", \"validation.csv\")\n",
    "    test_output_path = os.path.join(f\"{local_dir}/test\", \"test.csv\")\n",
    "    full_processed_output_path = os.path.join(f\"{local_dir}/full\", \"dataset.csv\")\n",
    "\n",
    "    logger.info(\"Saving train data to {}\".format(train_output_path))\n",
    "    train_df.to_csv(train_output_path, index=False)\n",
    "\n",
    "    logger.info(\"Saving validation data to {}\".format(validation_output_path))\n",
    "    val_df.to_csv(validation_output_path, index=False)\n",
    "\n",
    "    logger.info(\"Saving test data to {}\".format(test_output_path))\n",
    "    test_df.to_csv(test_output_path, index=False)\n",
    "\n",
    "    logger.info(\"Saving full processed data to {}\".format(full_processed_output_path))\n",
    "    dataset_df.to_csv(full_processed_output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload processing script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"process.py\",\n",
    "    Bucket=write_bucket,\n",
    "    Key=f\"{write_prefix}/scripts/process.py\",\n",
    ")\n",
    "\n",
    "# Define the SKLearnProcessor configuration\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=process_instance_type,\n",
    "    base_job_name=f\"{base_job_name_prefix}-processing\",\n",
    ")\n",
    "\n",
    "# Define pipeline processing step\n",
    "process_step = ProcessingStep(\n",
    "    name=\"DataProcessing\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=hospital_data_uri, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=f\"{processing_output_uri}/train_data\",\n",
    "            output_name=\"train_data\",\n",
    "            source=\"/opt/ml/processing/train\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=f\"{processing_output_uri}/validation_data\",\n",
    "            output_name=\"validation_data\",\n",
    "            source=\"/opt/ml/processing/val\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=f\"{processing_output_uri}/test_data\",\n",
    "            output_name=\"test_data\",\n",
    "            source=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            destination=f\"{processing_output_uri}/processed_data\",\n",
    "            output_name=\"processed_data\",\n",
    "            source=\"/opt/ml/processing/full\",\n",
    "        ),\n",
    "    ],\n",
    "    job_arguments=[\"--train-ratio\", \"0.8\", \"--validation-ratio\", \"0.1\", \"--test-ratio\", \"0.1\"],\n",
    "    code=f\"{scripts_uri}/preprocessing.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse job arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--num_round\", type=int, default=100)\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=6)\n",
    "    parser.add_argument(\"--eta\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.9)\n",
    "    parser.add_argument(\"--colsample_bytree\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--objective\", type=str, default=\"reg:squarederror\")\n",
    "    parser.add_argument(\"--eval_metric\", type=str, default=\"rmse\")\n",
    "    parser.add_argument(\"--nfold\", type=int, default=5)\n",
    "    parser.add_argument(\"--early_stopping_rounds\", type=int, default=10)\n",
    "\n",
    "    # SageMaker specific arguments\n",
    "    parser.add_argument(\"--train_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\n",
    "        \"--validation_data_dir\", type=str, default=os.environ.get(\"SM_CHANNEL_VALIDATION\")\n",
    "    )\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    parser.add_argument(\n",
    "        \"--output_data_dir\", type=str, default=os.environ.get(\"SM_OUTPUT_DATA_DIR\")\n",
    "    )\n",
    "    args, _ = parser.parse_known_args()\n",
    "    logger.info(f\"Received arguments {args}\")\n",
    "    return args\n",
    "\n",
    "\n",
    "def load_data(data_dir, filename):\n",
    "    \"\"\"\n",
    "    Load data from the specified directory\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory containing the data file\n",
    "        filename (str): Name of the data file\n",
    "\n",
    "    Returns:\n",
    "        tuple: Features DataFrame and labels Series\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(f\"{data_dir}/{filename}\")\n",
    "    features = data.drop(\"tiempo_total\", axis=1)\n",
    "    labels = data[\"tiempo_total\"]\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def calculate_regression_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate regression metrics\n",
    "\n",
    "    Args:\n",
    "        y_true: True values\n",
    "        y_pred: Predicted values\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing regression metrics\n",
    "    \"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2\n",
    "    }\n",
    "\n",
    "\n",
    "def train_model(args, dtrain, dvalidation):\n",
    "    \"\"\"\n",
    "    Train XGBoost regression model with cross-validation\n",
    "\n",
    "    Args:\n",
    "        args: Parsed command line arguments\n",
    "        dtrain: Training data as DMatrix\n",
    "        dvalidation: Validation data as DMatrix\n",
    "\n",
    "    Returns:\n",
    "        tuple: Trained model and metrics\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"max_depth\": args.max_depth,\n",
    "        \"eta\": args.eta,\n",
    "        \"objective\": args.objective,\n",
    "        \"subsample\": args.subsample,\n",
    "        \"colsample_bytree\": args.colsample_bytree,\n",
    "    }\n",
    "\n",
    "    # Run cross-validation\n",
    "    cv_results = xgb.cv(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=args.num_round,\n",
    "        nfold=args.nfold,\n",
    "        early_stopping_rounds=args.early_stopping_rounds,\n",
    "        metrics=[args.eval_metric],\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # Train final model\n",
    "    evallist = [(dtrain, 'train'), (dvalidation, 'validation')]\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=len(cv_results),\n",
    "        evals=evallist,\n",
    "        early_stopping_rounds=args.early_stopping_rounds\n",
    "    )\n",
    "\n",
    "    # Generate predictions\n",
    "    train_pred = model.predict(dtrain)\n",
    "    validation_pred = model.predict(dvalidation)\n",
    "\n",
    "    # Calculate metrics\n",
    "    train_metrics = calculate_regression_metrics(dtrain.get_label(), train_pred)\n",
    "    validation_metrics = calculate_regression_metrics(dvalidation.get_label(), validation_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"train\": train_metrics,\n",
    "        \"validation\": validation_metrics\n",
    "    }\n",
    "\n",
    "    return model, metrics\n",
    "\n",
    "\n",
    "def save_model_artifacts(model, metrics, args):\n",
    "    \"\"\"\n",
    "    Save model artifacts and metrics\n",
    "\n",
    "    Args:\n",
    "        model: Trained XGBoost model\n",
    "        metrics (dict): Model metrics\n",
    "        args: Parsed command line arguments\n",
    "    \"\"\"\n",
    "    metrics_data = {\n",
    "        \"hyperparameters\": {\n",
    "            \"max_depth\": args.max_depth,\n",
    "            \"eta\": args.eta,\n",
    "            \"objective\": args.objective,\n",
    "            \"subsample\": args.subsample,\n",
    "            \"colsample_bytree\": args.colsample_bytree,\n",
    "        },\n",
    "        \"regression_metrics\": {\n",
    "            \"validation\": {\n",
    "                \"rmse\": metrics[\"validation\"][\"rmse\"],\n",
    "                \"mae\": metrics[\"validation\"][\"mae\"],\n",
    "                \"r2\": metrics[\"validation\"][\"r2\"]\n",
    "            },\n",
    "            \"train\": {\n",
    "                \"rmse\": metrics[\"train\"][\"rmse\"],\n",
    "                \"mae\": metrics[\"train\"][\"mae\"],\n",
    "                \"r2\": metrics[\"train\"][\"r2\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_location = os.path.join(args.output_data_dir, \"metrics.json\")\n",
    "    with open(metrics_location, \"w\") as f:\n",
    "        json.dump(metrics_data, f)\n",
    "\n",
    "    # Save model\n",
    "    model_location = os.path.join(args.model_dir, \"xgboost-model\")\n",
    "    with open(model_location, \"wb\") as f:\n",
    "        joblib.dump(model, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Main training function\"\"\"\n",
    "    args = parse_args()\n",
    "\n",
    "    # Load data\n",
    "    train_features, train_labels = load_data(args.train_data_dir, \"train.csv\")\n",
    "    validation_features, validation_labels = load_data(args.validation_data_dir, \"validation.csv\")\n",
    "\n",
    "    # Create DMatrix objects\n",
    "    dtrain = xgb.DMatrix(train_features, label=train_labels)\n",
    "    dvalidation = xgb.DMatrix(validation_features, label=validation_labels)\n",
    "\n",
    "    # Train model\n",
    "    model, metrics = train_model(args, dtrain, dvalidation)\n",
    "\n",
    "    # Log metrics\n",
    "    logger.info(\"Training metrics:\")\n",
    "    logger.info(f\"RMSE: {metrics['train']['rmse']:.4f}\")\n",
    "    logger.info(f\"MAE: {metrics['train']['mae']:.4f}\")\n",
    "    logger.info(f\"R2: {metrics['train']['r2']:.4f}\")\n",
    "\n",
    "    logger.info(\"\\nValidation metrics:\")\n",
    "    logger.info(f\"RMSE: {metrics['validation']['rmse']:.4f}\")\n",
    "    logger.info(f\"MAE: {metrics['validation']['mae']:.4f}\")\n",
    "    logger.info(f\"R2: {metrics['validation']['r2']:.4f}\")\n",
    "\n",
    "    # Save artifacts\n",
    "    save_model_artifacts(model, metrics, args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set XGBoost model hyperparameters\n",
    "hyperparams = {\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"100\",\n",
    "    \"max_depth\": \"6\",\n",
    "    \"subsample\": \"0.9\",\n",
    "    \"colsample_bytree\": \"0.8\",\n",
    "    \"eta\": \"0.3\",\n",
    "}\n",
    "\n",
    "# Set XGBoost estimator\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point=\"train.py\",\n",
    "    output_path=estimator_output_uri,\n",
    "    code_location=estimator_output_uri,\n",
    "    hyperparameters=hyperparams,\n",
    "    role=sagemaker_role,\n",
    "    # Fetch instance type and count from pipeline parameters\n",
    "    instance_count=train_instance_count,\n",
    "    instance_type=train_instance_type,\n",
    "    framework_version=\"1.3-1\",\n",
    ")\n",
    "\n",
    "# Access the location where the preceding processing step saved train and validation datasets\n",
    "# Pipeline step properties can give access to outputs which can be used in succeeding steps\n",
    "\n",
    "# Set pipeline training step\n",
    "train_step = TrainingStep(\n",
    "    name=\"TrainModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"csv\",\n",
    "            s3_data_type=\"S3Prefix\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"validation_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"csv\",\n",
    "            s3_data_type=\"S3Prefix\",\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker model\n",
    "model = sagemaker.model.Model(\n",
    "    image_uri=training_image,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "# Specify model deployment instance type\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=deploy_instance_type_param)\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"CreateModel\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile evaluate.py\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model from training script\"\"\"\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    logger.debug(\"Loading xgboost model.\")\n",
    "    # The name of the file should match how the model was saved in the training script\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Loada data from input path\"\"\"\n",
    "    logger.debug(\"Reading test data.\")\n",
    "    input_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df_test = pd.read_csv(input_path)\n",
    "\n",
    "    # Extract test set target column\n",
    "    y_test = df_test[\"tiempo_total\"]\n",
    "\n",
    "    # Extract test set feature columns\n",
    "    X = df_test.drop(\"tiempo_total\", axis=1)\n",
    "    x_test = xgb.DMatrix(X)\n",
    "    return x_test, y_test\n",
    "\n",
    "\n",
    "def calculate_regression_metrics(x_test, y_test, model):\n",
    "    \"\"\"Calculate regression metrics\"\"\"\n",
    "    logger.info(\"Generating predictions for test data.\")\n",
    "    pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate model evaluation score\n",
    "    logger.debug(\"Calculating regression metrics.\")\n",
    "    rmse = root_mean_squared_error(y_test, pred)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    r2 = r2_score(y_test, pred)\n",
    "\n",
    "    metric_dict = {\"regression_metrics\": {\"test\": {\"rmse\": rmse, \"mae\": mae, \"r2\": r2}}}\n",
    "    return metric_dict\n",
    "\n",
    "\n",
    "def save_model_evaluation(metric_dict):\n",
    "    \"\"\"Save model evaluation metrics\"\"\"\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    logger.info(f\"Writing evaluation report with regression metrics: {metric_dict}\")\n",
    "    evaluation_location = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_location, \"w\") as f:\n",
    "        json.dump(metric_dict, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Main evaluate function\"\"\"\n",
    "    # Load model\n",
    "    model = load_data()\n",
    "\n",
    "    # Load data\n",
    "    x_test, y_test = load_data()\n",
    "\n",
    "    # Calculate regression metric\n",
    "    test_metrics = calculate_regression_metrics(x_test, y_test, model)\n",
    "\n",
    "    # Save model evaluation metrics\n",
    "    save_model_evaluation(test_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload model evaluation script to S3\n",
    "s3_client.upload_file(\n",
    "    Filename=\"evaluate.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/evaluate.py\"\n",
    ")\n",
    "\n",
    "eval_processor = ScriptProcessor(\n",
    "    image_uri=training_image,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=predictor_instance_type,\n",
    "    instance_count=predictor_instance_count,\n",
    "    base_job_name=f\"{base_job_name_prefix}-model-eval\",\n",
    "    sagemaker_session=sess,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"HospitalWaitTimePredReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "# Set model evaluation step\n",
    "evaluation_step = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    processor=eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where train step saved model artifacts\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            # Fetch S3 location where processing step saved test data\n",
    "            source=process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test_data\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            destination=f\"{model_eval_output_uri}\",\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/evaluation\",\n",
    "        ),\n",
    "    ],\n",
    "    code=f\"s3://{write_bucket}/{write_prefix}/scripts/evaluate.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define register model step\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=xgb_estimator,\n",
    "    # Fetching S3 location where train step saved model artifacts\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[predictor_instance_type],\n",
    "    transform_instances=[predictor_instance_type],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status_param,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_deployer.py\n",
    "\n",
    "\"\"\"\n",
    "Lambda function creates an endpoint configuration and deploys a model to real-time endpoint. \n",
    "Required parameters for deployment are retrieved from the event object\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    sm_client = boto3.client(\"sagemaker\")\n",
    "\n",
    "    # Details of the model created in the Pipeline CreateModelStep\n",
    "    model_name = event[\"model_name\"]\n",
    "    model_package_arn = event[\"model_package_arn\"]\n",
    "    endpoint_config_name = event[\"endpoint_config_name\"]\n",
    "    endpoint_name = event[\"endpoint_name\"]\n",
    "    role = event[\"role\"]\n",
    "    instance_type = event[\"instance_type\"]\n",
    "    instance_count = event[\"instance_count\"]\n",
    "    primary_container = {\"ModelPackageName\": model_package_arn}\n",
    "\n",
    "    # Create model\n",
    "    model = sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer=primary_container,\n",
    "        ExecutionRoleArn=role\n",
    "    )\n",
    "\n",
    "    # Create endpoint configuration\n",
    "    create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"Alltraffic\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InitialInstanceCount\": instance_count,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialVariantWeight\": 1\n",
    "        }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create endpoint\n",
    "    create_endpoint_response = sm_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        EndpointConfigName=endpoint_config_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function name must contain sagemaker\n",
    "function_name = \"sagemaker-hospital-wait-time-lambda-step\"\n",
    "# Define Lambda helper class can be used to create the Lambda function required in the Lambda step\n",
    "func = Lambda(\n",
    "    function_name=function_name,\n",
    "    execution_role_arn=sagemaker_role,\n",
    "    script=\"lambda_deployer.py\",\n",
    "    handler=\"lambda_deployer.lambda_handler\",\n",
    "    timeout=600,\n",
    "    memory_size=10240,\n",
    ")\n",
    "\n",
    "# The inputs used in the lambda handler are passed through the inputs argument in the\n",
    "# LambdaStep and retrieved via the `event` object within the `lambda_handler` function\n",
    "\n",
    "lambda_deploy_step = LambdaStep(\n",
    "    name=\"LambdaStepRealTimeDeploy\",\n",
    "    lambda_func=func,\n",
    "    inputs={\n",
    "        \"model_name\": pipeline_model_name,\n",
    "        \"endpoint_config_name\": endpoint_config_name,\n",
    "        \"endpoint_name\": endpoint_name,\n",
    "        \"model_package_arn\": register_step.steps[0].properties.ModelPackageArn,\n",
    "        \"role\": sagemaker_role,\n",
    "        \"instance_type\": deploy_instance_type_param,\n",
    "        \"instance_count\": deploy_instance_count_param,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on test set\n",
    "cond_gte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=evaluation_step.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.test.rmse\",\n",
    "    ),\n",
    "    right=0.4,  # Threshold to compare model performance against\n",
    ")\n",
    "condition_step = ConditionStep(\n",
    "    name=\"CheckEvaluation\",\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[\n",
    "        create_model_step,\n",
    "        register_step,\n",
    "        lambda_deploy_step,\n",
    "    ],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Pipeline with all component steps and parameters\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        process_instance_type_param,\n",
    "        train_instance_type_param,\n",
    "        train_instance_count_param,\n",
    "        deploy_instance_type_param,\n",
    "        deploy_instance_count_param,\n",
    "        model_approval_status_param,\n",
    "    ],\n",
    "    steps=[process_step, train_step, evaluation_step, condition_step],\n",
    "    sagemaker_session=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new or update existing Pipeline\n",
    "pipeline.upsert(role_arn=sagemaker_role)\n",
    "\n",
    "# Full Pipeline description\n",
    "pipeline_definition = json.loads(pipeline.describe()[\"PipelineDefinition\"])\n",
    "pipeline_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute Pipeline\n",
    "start_response = pipeline.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
